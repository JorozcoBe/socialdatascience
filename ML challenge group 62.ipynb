{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "D:\\Anocondo\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "D:\\Anocondo\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "D:\\Anocondo\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "D:\\Anocondo\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "D:\\Anocondo\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "D:\\Anocondo\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "D:\\Anocondo\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "D:\\Anocondo\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "D:\\Anocondo\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "D:\\Anocondo\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "D:\\Anocondo\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "D:\\Anocondo\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "# load necessary packages\n",
    "import keras\n",
    "import sklearn\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, Dropout, MaxPooling2D\n",
    "from keras.optimizers import Adam, SGD\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as pt\n",
    "from matplotlib import cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data\n",
    "with np.load('training-dataset.npz') as data:\n",
    "    img = data['x']\n",
    "    lbl = data['y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(99840, 784)\n",
      "(24960, 784)\n",
      "(99840,)\n",
      "(24960,)\n"
     ]
    }
   ],
   "source": [
    "# split datasets\n",
    "X_train, X_val, Y_train, Y_val= train_test_split(img, lbl, test_size=0.2)\n",
    "print(X_train.shape)\n",
    "print(X_val.shape)\n",
    "print(Y_train.shape)\n",
    "print(Y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "                     metric_params=None, n_jobs=None, n_neighbors=500, p=2,\n",
       "                     weights='uniform')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#1 sklearn KNN trial\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "clf = KNeighborsClassifier(n_neighbors=500)\n",
    "clf.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6942708333333333\n"
     ]
    }
   ],
   "source": [
    "#1 sklearn KNN trial accuracy\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(accuracy_score(Y_val, clf.predict(X_val)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Perceptron(alpha=0.0001, class_weight=None, early_stopping=False, eta0=1.0,\n",
       "           fit_intercept=True, max_iter=1000, n_iter_no_change=5, n_jobs=None,\n",
       "           penalty=None, random_state=0, shuffle=True, tol=0.001,\n",
       "           validation_fraction=0.1, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#2 sklearn perceptron base\n",
    "from sklearn.linear_model import Perceptron\n",
    "baseline = Perceptron()\n",
    "baseline.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5059294871794872\n"
     ]
    }
   ],
   "source": [
    "#2 sklearn perceptron baseline accuracy\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(accuracy_score(Y_val, baseline.predict(X_val)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784,)\n",
      "(28, 28)\n"
     ]
    }
   ],
   "source": [
    "# reshape and check the shape of input variables\n",
    "d = img[1]\n",
    "print(d.shape)\n",
    "d = d.reshape(28,28)\n",
    "print(d.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# visualize the input variables\n",
    "pt.imshow(d, cmap = \"gray\")\n",
    "pt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = inf\n",
      "Iteration 2, loss = 122042191276514145378771792113613106829993532076552482950937809527984279736729922262452292785354670426030080.00000000\n",
      "Iteration 3, loss = 121980697242896724408903350276954855348480801789837758461184564780993676641885306533755512913662254851817472.00000000\n",
      "Iteration 4, loss = 121919234194597948370324250528724687978951390601778878768213497222164334030603521755103905778696889294651392.00000000\n",
      "Iteration 5, loss = 121857802116005012059426161626328518259070895059585373809887061104710571240740494377137770569343697701306368.00000000\n",
      "Iteration 6, loss = 121796400991512736935296148643079248787162316422301750221733595947169089319994533654083720407115754658332672.00000000\n",
      "Iteration 7, loss = 121735030805524239267516910679633067272391858936844316218580448960489179679973719880821004720593713546395648.00000000\n",
      "Iteration 8, loss = 121673691542450864951015538160434685944506900737900027947565377738072703739701525052679916818651208671559680.00000000\n",
      "Iteration 9, loss = 121612383186710690247584930651957847977809324545599955607398812126693624726246088040805168074639104284819456.00000000\n",
      "Iteration 10, loss = 121551105722730721784738238107678497412498999793933219034229013437648694706405517323959632330028172651266048.00000000\n",
      "Iteration 11, loss = 121489859134945266926927793279205764398123055128218146526927141697756945674348407483484535225044147332186112.00000000\n",
      "Iteration 12, loss = 121428643407796618219633160103606951391327183956164388140467527403939903294804822275416174679801000806580224.00000000\n",
      "Iteration 13, loss = 121367458525734466722999949371414687843497382546962532863030295733581403694616880568671589053329563652718592.00000000\n",
      "Iteration 14, loss = 121306304473217124233425619418278691268172995658180739497037566106277975147008368973828415145601234586566656.00000000\n",
      "Iteration 15, loss = 121245181234710040321368704619094963812585554508463491190162863382248374961337611853538660488405878948495360.00000000\n",
      "Iteration 16, loss = 121184088794687497145281125680429567604471533380161590257034647921294118751951334167770106441491373289897984.00000000\n",
      "Iteration 17, loss = 121123027137630099823285345553660342026984229265437742770175319149338696713151056876044999761741587427622912.00000000\n",
      "Iteration 18, loss = 121061996248027367542936266901278637181609418595287416027797958629338382789844675710453351567466649725435904.00000000\n",
      "Iteration 19, loss = 121000996110376886154255076950677426214758978938223841142954565031801970043119529753032235790333174800711680.00000000\n",
      "Iteration 20, loss = 120940026709182548170645694498172769380696103297546864574843926765870221024894159250322793652450121069101056.00000000\n",
      "Iteration 21, loss = 120879088028957812026456905086741843554636755116400630478241487477316885602637272623449855005098684180332544.00000000\n",
      "Iteration 22, loss = 120818180054222524300859279207728363457475749647842340412805227187997708580266753633294307523470650918305792.00000000\n",
      "Iteration 23, loss = 120757302769505543420182691118921695413833425233913687634366704021741249047048429823607191884345963416911872.00000000\n",
      "Iteration 24, loss = 120696456159342295214744717461253335217229552051847595332858655080848117009056847263449985762045299083182080.00000000\n",
      "Iteration 25, loss = 120635640208276972917705078503770078051426814242482152218180155657241660420860573278998348232072748666454016.00000000\n",
      "Iteration 26, loss = 120574854900861233462040784072538806685190227908237539580424673033267558055634609649510271235623858884050944.00000000\n",
      "Iteration 27, loss = 120514100221654490813726725716642914128466272063571222701328754376513911109385099638233245500071322830503936.00000000\n",
      "Iteration 28, loss = 120453376155224078934613783467069205108037961385230834971743520014711294092185276342908241605901474649341952.00000000\n",
      "Iteration 29, loss = 120392682686144991041823855022488853706476729811849563303680274195892673402197945334963340279615898058227712.00000000\n",
      "Iteration 30, loss = 120332019798999521089418020879706220115701270493389803071873219883613295364956398288404974046471138071019520.00000000\n",
      "Iteration 31, loss = 120271387478378225249377374211091569343342465017635174406861269078860986491656508247791415525404019561332736.00000000\n",
      "Iteration 32, loss = 120210785708879791541300535457471550032219325209988214901010848200136112766167970947829592575482451525107712.00000000\n",
      "Iteration 33, loss = 120150214475110322795739982589026827967456673032357689491641327673891355723596132071157571601384850524733440.00000000\n",
      "Iteration 34, loss = 120089673761683287765338619077626015633788618758082725225783573952061697182911202743193363233319192287510528.00000000\n",
      "Iteration 35, loss = 120029163553220417420659361070703630331661461098323673906273162524014699148745975459906818195173732400496640.00000000\n",
      "Iteration 36, loss = 119968683834350922728370224948602967094089338000848268327887210197030259533463272005400518183221831885914112.00000000\n",
      "Iteration 37, loss = 119908234589712032428742079628902873556315966725866139865000302903470780098234573008572913387012389618057216.00000000\n",
      "Iteration 38, loss = 119847815803948259702697166151426693315366816469890839942962775963730937442478250365850407688155616300236800.00000000\n",
      "Iteration 39, loss = 119787427461712428837941170259229745226676066692333009978171121718108003618646041849362439382022950897057792.00000000\n",
      "Iteration 40, loss = 119727069547664257451923693596283280565887474188300289556065998035171403378470300457556845895399556989845504.00000000\n",
      "Iteration 41, loss = 119666742046472523898117073600670272655066840666960675193503088867933391026402107476902811976739701911453696.00000000\n",
      "Iteration 42, loss = 119606444942812688007996024824836655318137950597850412223669302440308307407566426564532746118893934538129408.00000000\n",
      "Iteration 43, loss = 119546178221368227386638114358463914981254167762947644559349016101292997215896880222374925307988341598191616.00000000\n",
      "Iteration 44, loss = 119485941866830001857499145468810174917743651530686166634785252552856610518315551230187968862372218452049920.00000000\n",
      "Iteration 45, loss = 119425735863896889017637773960369111003164140581941671463818503622050801230553181212526364593670398312185856.00000000\n",
      "Iteration 46, loss = 119365560197275442015671483981207458616427300134500693993196594383719620245553679102682108566216613432393728.00000000\n",
      "Iteration 47, loss = 119305414851680150292381558837184055000846838343465225690529072400339596859451638503491074804151886582644736.00000000\n",
      "Iteration 48, loss = 119245299811832787729200653956402233361518215300241178074401234622419535206628544285317091023678552361926656.00000000\n",
      "Iteration 49, loss = 119185215062463211166315520007755640096028499508777514889984444435510763626768919503521445861049081090015232.00000000\n",
      "Iteration 50, loss = 119125160588308887810320493300156220515056656909832888168368801162996987080276075182001343778450244259282944.00000000\n",
      "Iteration 51, loss = 119065136374114927826793117134311599137504565431026215050057438721074297326519299983290701277390413469057024.00000000\n",
      "Iteration 52, loss = 119005142404633872488552603016172107772145420411507928432253584361381106765227127352904971511682117352292352.00000000\n",
      "Iteration 53, loss = 118945178664626085286567286878259349059195909202566898850792142530758270575453611560550698848089628787867648.00000000\n",
      "Iteration 54, loss = 118885245138859491189349658265447154109268094767225819892185305631409005289600810339319149159230573425721344.00000000\n",
      "Iteration 55, loss = 118825341812110570716512811564171681504865359456783801310198662997789199229958069823760594331888547184771072.00000000\n",
      "Iteration 56, loss = 118765468669161752532720737860240993689901241006770223146307308498206598248926850941820615196029201504272384.00000000\n",
      "Iteration 57, loss = 118705625694805503815928804586896284008641758568292127078230321734812083099043033012490026304906259605749760.00000000\n",
      "Iteration 58, loss = 118645812873839946555962683710754227010583955726231633659947598948788300458729903117247785234469809821843456.00000000\n",
      "Iteration 59, loss = 118586030191071660516021787984661683831723151803595549142209537348630538261556467728729466934674013951098880.00000000\n",
      "Iteration 60, loss = 118526277631314607677683766339042152455229461709846330297225179495806389814080193597400988687696242424152064.00000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 61, loss = 118466555179390588536963202806779091843281999641610159947582394474475896291307663132969757095359259384741888.00000000\n",
      "Iteration 62, loss = 118406862820129176919160373819661161346806849982575793319259280581531526380200195064183075652356624820994048.00000000\n",
      "Iteration 63, loss = 118347200538367068127348821172834614802856776302483021571738192226997972714730051036812220478506715878260736.00000000\n",
      "Iteration 64, loss = 118287568318949284867673342040566371454458759708994014267294790168292528471026453407381900214082787430367232.00000000\n",
      "Iteration 65, loss = 118227966146728232064656969774699987391814576896226591489663380515804792198207084810247165831581302983229440.00000000\n",
      "Iteration 66, loss = 118168394006563191676278842952106260977770074618970283079875285580706510055546648271031069328218801192304640.00000000\n",
      "Iteration 67, loss = 118108851883322245655935865129548670260545028145670361222422464518772176329061057742571640315782532989714432.00000000\n",
      "Iteration 68, loss = 118049339761880581138510394551259597626920384655797845623553983271418504162653575259680482924490916997627904.00000000\n",
      "Iteration 69, loss = 117989857627121028217867996453267104671900003315684021098931942372874593499193440495805913323883971945168896.00000000\n",
      "Iteration 70, loss = 117930405463934402168901467257057425297586308054051494220321612827472037952111362299091319963389955479175168.00000000\n",
      "Iteration 71, loss = 117870983257218330114808465905589275088463763760193427669796684530364558388500690568746499890382602527309824.00000000\n",
      "Iteration 72, loss = 117811590991878668804131042665609894149598009211173632061741257759158445682870166904432294032528628942635008.00000000\n",
      "Iteration 73, loss = 117752228652828314981745459787780666334353825994980511885307942267142189211121501647581525409151320399544320.00000000\n",
      "Iteration 74, loss = 117692896224988623165901720308993162082594271437727157326419848774916919604302124530783634413578036038336512.00000000\n",
      "Iteration 75, loss = 117633593693288069352623092629496546320309082051577694504504344116215991440470408203652034064260952156536832.00000000\n",
      "Iteration 76, loss = 117574321042662267311993921188786268607180180809772573884240200565394988333819263067874508111470711676076032.00000000\n",
      "Iteration 77, loss = 117515078258055565624365072702695697598001390099106834626778229931452221668788431112152225493274071926833152.00000000\n",
      "Iteration 78, loss = 117455865324418375089152985317650936841935238615782805063208793849667895983794932963934450837775392029278208.00000000\n",
      "Iteration 79, loss = 117396682226710411686113993112520164148048909095442496632244519952115121706828440064447767695152876863094784.00000000\n",
      "Iteration 80, loss = 117337528949897306947479705513766080891688725107907614236813241749738856617743548978213272345379487893946368.00000000\n",
      "Iteration 81, loss = 117278405478953785734080089093740490789754069687110796534752117492906898227360774227874204603484589273382912.00000000\n",
      "Iteration 82, loss = 117219311798860651422098492531276622601078539451417259765585004585758942289601503809872298081228796410724352.00000000\n",
      "Iteration 83, loss = 117160247894607539975711650836877765065000674082898539335790695932017442863375526317965060218330735926312960.00000000\n",
      "Iteration 84, loss = 117101213751191127355430511004961350669158156074550764526616493911087052508985599087681980546165604261822464.00000000\n",
      "Iteration 85, loss = 117042209353615895443627333780627392585066652656483213846194228770084861476936405443693241702366692637671424.00000000\n",
      "Iteration 86, loss = 116983234686892828341510839588563272862879233793895242089768312424639990578046967895777870895333429678702592.00000000\n",
      "Iteration 87, loss = 116924289736042133849190591406454997074826087116299445101469676149724441067090985592240223722070842433077248.00000000\n",
      "Iteration 88, loss = 116865374486090521985612611891575939671774802988298495012541832888822244759156496866493500350350095353380864.00000000\n",
      "Iteration 89, loss = 116806488922072280543554887989440393722553854661254175416652732868269799911803138350386020560489355130437632.00000000\n",
      "Iteration 90, loss = 116747633029029209904476128230248810323690569173186227722906166287295850868567765633999631318579192824594432.00000000\n",
      "Iteration 91, loss = 116688806792010883779120733543106840960459243749176965743796152557861569484941970626456076483580975340584960.00000000\n",
      "Iteration 92, loss = 116630010196074551429791933200693196621488102151219545224724044339720524593630512545614608167164968624455680.00000000\n",
      "Iteration 93, loss = 116571243226284909522322435356817985732842192827863926079538436955558610756820990227366413239999245123059712.00000000\n",
      "Iteration 94, loss = 116512505867713727311454781500982840764016721588635238920350733858723931216341160919475456877795746038939648.00000000\n",
      "Iteration 95, loss = 116453798105441427380758982019583862541791257281486760997008631399757291538647639281467098910594279646691328.00000000\n",
      "Iteration 96, loss = 116395119924554641199460914812608098115403640539006654435023719702891436246286672127068377818715101216112640.00000000\n",
      "Iteration 97, loss = 116336471310147561714330611392394823004487087605516904410584876847721471215152509733391053588331818788061184.00000000\n",
      "Iteration 98, loss = 116277852247323361126721785685951584037269323264273410972560260360835306428240156490316242993818896819027968.00000000\n",
      "Iteration 99, loss = 116219262721191045709024373588437000870429676757585325575297487010242673774791567735767585005877309015588864.00000000\n",
      "Iteration 100, loss = 116160702716867411359201814069803583696959954789849660488281552104175737745127029961759013594772474395885568.00000000\n",
      "Iteration 101, loss = 116102172219478102859498743107562592837922414404195035559699535777939426223678825092670634665421472656261120.00000000\n",
      "Iteration 102, loss = 116043671214155039062966906896370995427099613528486109160391009250490626407463246547285825199796428356976640.00000000\n",
      "Iteration 103, loss = 115985199686037700300202205243237987068669485702321362711872831696148586846844591207772683685722818829352960.00000000\n",
      "Iteration 104, loss = 115926757620273226157071555622857132722598383727184831156822046209534947980276728429986418757041370978844672.00000000\n",
      "Iteration 105, loss = 115868345002016611030166621288270648475427166966749563900041671735453803203786238064074126473931854891188224.00000000\n",
      "Iteration 106, loss = 115809961816429286349764282468553358702072068419677532986458711577261256117219659805090155040565580187828224.00000000\n",
      "Iteration 107, loss = 115751608048681597615573859103893598495781800523463471686690851175346194071029908120656617022540115037126656.00000000\n",
      "Iteration 108, loss = 115693283683950115509248349323959339319828854775262286558766819071279448460883124467647701742427124069826560.00000000\n",
      "Iteration 109, loss = 115634988707419868485814492044648739223983998410353569857485844773944491937591169694094215896802345382576128.00000000\n",
      "Iteration 110, loss = 115576723104282680552316078027443747792834532349562181536208427355556919316504950864042976673493344885669888.00000000\n",
      "Iteration 111, loss = 115518486859737953489628862322067233228928640400472093010719503171184691855446973339975920872874690727641088.00000000\n",
      "Iteration 112, loss = 115460279958992177963826243989822277925810171006667734806813966034047984581003495731298161707058469279891456.00000000\n",
      "Iteration 113, loss = 115402102387260302414355362878242150863521248349858223147057210724679613774906496038571429762160840379727872.00000000\n",
      "Iteration 114, loss = 115343954129764298980709760142885574622837632147651979717035992171804589131157326169079039609260884218609664.00000000\n",
      "Iteration 115, loss = 115285835171733163502429378247336725385268717653556731665358425452340113756026711936825889804400601342148608.00000000\n",
      "Iteration 116, loss = 115227745498403502185461745295198078245415797557889894426551361581035767376504167124352794729556293468553216.00000000\n",
      "Iteration 117, loss = 115169685095019906416781620575530280604978729313360205191244822042743623390140674688895641046592501233745920.00000000\n",
      "Iteration 118, loss = 115111653946833437209624601705203968449163758557715900613687111335847775576554367102698363774752603743715328.00000000\n",
      "Iteration 119, loss = 115053652039103743720902512494429485530199464867032210338874229545964999683497854909564700160833249307787264.00000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 120, loss = 114995679357096798067197718998228952855731250527196267769696115824834046038678544652852852848767079500742656.00000000\n",
      "Iteration 121, loss = 114937735886086671620134493188303494773461633511663546947375925086726194264230475895968883509230521085329408.00000000\n",
      "Iteration 122, loss = 114879821611354720191988479158598731592874883730193441964112561633944998820534579469846807504961312653312000.00000000\n",
      "Iteration 123, loss = 114821936518189763294852610560080371208457603054127439492299321759216344986578220658502973064388648764440576.00000000\n",
      "Iteration 124, loss = 114764080591887399696549062213409222901947419765327005491143621991108235116764216126919340800499902324932608.00000000\n",
      "Iteration 125, loss = 114706253817751669641985939049587592394014730608753003204876230498011307731517507096184090593592870239731712.00000000\n",
      "Iteration 126, loss = 114648456181093229668922480410425985316925885989633391047070542409258267535442537817847033641587033088458752.00000000\n",
      "Iteration 127, loss = 114590687667230460755540186008974037248997684675182834599448734084444231477727704357346683715191718893584384.00000000\n",
      "Iteration 128, loss = 114532948261489289061276898494744922091376793769324534082663120511036672771785811408455877984275958981459968.00000000\n",
      "Iteration 129, loss = 114475237949202860001070589935941549116729603212184456121353168754575319112782175592273814883995498638737408.00000000\n",
      "Iteration 130, loss = 114417556715711212319603148301682760017932080279585565509202511408872050895162728756218091980919807768788992.00000000\n",
      "Iteration 131, loss = 114359904546362761053491148967874132336530931610349070677929535397601364322901147963610933682154182404997120.00000000\n",
      "Iteration 132, loss = 114302281426512781976519461859559795739151426628942601404802494517729898121004535833992167312830344263106560.00000000\n",
      "Iteration 133, loss = 114244687341524112340017110514136108362814210371567610517766930521586652367827998450286341703933367829921792.00000000\n",
      "Iteration 134, loss = 114187122276766238280739874231585008551266896084743222837605310777422703504153334595980433217457310201282560.00000000\n",
      "Iteration 135, loss = 114129586217617022227378219718675170496927836377986806823134856983351743780290088270466045055932054603038720.00000000\n",
      "Iteration 136, loss = 114072079149461366604956825666089412140514526671740324807941300313169663947940772214906762513544054082371584.00000000\n",
      "Iteration 137, loss = 114014601057691148649683340044869934580781576097269179353390284106394532903706488572036560549361733639077888.00000000\n",
      "Iteration 138, loss = 113957151927706295963943884715071871813844187648331248423939219482606933571244187999486078829116355059384320.00000000\n",
      "Iteration 139, loss = 113899731744913026517219502429784754796103372478445736918445157972529413275918427484339626202287874460614656.00000000\n",
      "Iteration 140, loss = 113842340494726211607818704838992582841244504779559996373501444004077221667722621425442906172684115032145920.00000000\n",
      "Iteration 141, loss = 113784978162566752160539953671494709862190650502978090672146673176467491372380273190288921720763202819588096.00000000\n",
      "Iteration 142, loss = 113727644733863990577267640766431986210797644063103480982442792728739991181637010702476895094293687867277312.00000000\n",
      "Iteration 143, loss = 113670340194053983330464156429085603035910317184758454112277272825816586606159533926370068498827700697300992.00000000\n",
      "Iteration 144, loss = 113613064528580592814453204715419332170251486329878446096422111500331580846816468815472377246175466610491392.00000000\n",
      "Iteration 145, loss = 113555817722894639938453648285867638457015574396197044785682069623151450160251435889807748206335533393117184.00000000\n",
      "Iteration 146, loss = 113498599762454572274379746116771975800054408993282814728527793330465052512950438472986422181934399471222784.00000000\n",
      "Iteration 147, loss = 113441410632725388501845648891727237424553742290870261995783958409443291699084603578878678866445649076813824.00000000\n",
      "Iteration 148, loss = 113384250319180610258990490784113263732459631296919732002034338924529733124465156952713007754121537776517120.00000000\n",
      "Iteration 149, loss = 113327118807300408069380161729895475332445101229709242154699634556090018557330017040280325870223303745994752.00000000\n",
      "Iteration 150, loss = 113270016082571992452914763648953436581482320118441403735969054460171988267310071029143532331668754982240256.00000000\n",
      "Iteration 151, loss = 113212942130490526517946008294847503848511006204659572960640679607945966016350485611459802321876639465603072.00000000\n",
      "Iteration 152, loss = 113155896936558343739462304812161698431294078741034007212258295066184514780778154901561477969471470735196160.00000000\n",
      "Iteration 153, loss = 113098880486284230922425089997401340063535337890249174926236819587702212829863527693738551653762951333019648.00000000\n",
      "Iteration 154, loss = 113041892765185807459789186978741808459443526878695863704946107424896396737865453379596789582018795012096000.00000000\n",
      "Iteration 155, loss = 112984933758786673482135936935507767489768558867045708261959816018996270787402085814238063118076069980143616.00000000\n",
      "Iteration 156, loss = 112928003452618137264181130742374320823745288102931762067253234714003446416551935831602548091868042420551680.00000000\n",
      "Iteration 157, loss = 112871101832219247817350630321144392224224524470999074170697583411671952397101060914569521012811475426738176.00000000\n",
      "Iteration 158, loss = 112814228883135654149633621328540415218087524241134500379759559647207878595891426741429491601251166299619328.00000000\n",
      "Iteration 159, loss = 112757384590920746005729360468412643425831499320236892081801791513569732213472545639412070257015409250140160.00000000\n",
      "Iteration 160, loss = 112700568941134724978641966966083812150335702575775656774495325994538147704056570986813275977877975770071040.00000000\n",
      "Iteration 161, loss = 112643781919345826731266223260000899444274473462687388946855828526466268459787909841775394361581824670433280.00000000\n",
      "Iteration 162, loss = 112587023511128935811923667551196463567049119646226355080735890161629314235237661963003138536878897371283456.00000000\n",
      "Iteration 163, loss = 112530293702066693801933719763719573019242411703684105649631181837544927207808068593190720418698281669885952.00000000\n",
      "Iteration 164, loss = 112473592477748211908876638149429284884943508397391690590655657505262769936968520490040400279347903833571328.00000000\n",
      "Iteration 165, loss = 112416919823770358373330562683201166489423031401717443832566351001074927405019549895243937177312836506877952.00000000\n",
      "Iteration 166, loss = 112360275725737253283949384110377900824602339777283042531601746894325749254261390647920247649752165229002752.00000000\n",
      "Iteration 167, loss = 112303660169260855243823928278762131861411791871873889894379156277052032995443392246429737553470369252245504.00000000\n",
      "Iteration 168, loss = 112247073139958973223369053680196266546798857767353928944742498810202403134685449972225733048292086546169856.00000000\n",
      "Iteration 169, loss = 112190514623457857670085548916864208266643776008686499991559045794548120344129583662867779561352379079065600.00000000\n",
      "Iteration 170, loss = 112133984605390521990915633084756271646512304276329634218763039439212557282209669701830636797650837698510848.00000000\n",
      "Iteration 171, loss = 112077483071397394403755382809216788454276010387247590155241665961271402158595234420520203007797560818532352.00000000\n",
      "Iteration 172, loss = 112021010007126203863440057513723276567153721369733834792605010293322623110326290252920728241157608149352448.00000000\n",
      "Iteration 173, loss = 111964565398231018580763269542453721265346594236919027292104245761576167943521240467621326050925682133827584.00000000\n",
      "Iteration 174, loss = 111908149230375309724585391227360322974353483691525240393095707653974920888612708157290817552532227777101824.00000000\n",
      "Iteration 175, loss = 111851761489227192905792837528671974175842817818481744182873289593113216576257844403961484269097788230336512.00000000\n",
      "Iteration 176, loss = 111795402160463502249250735007066796299029416841247109033455775408738112318237037041628594804845371585462272.00000000\n",
      "Iteration 177, loss = 111739071229767900024416883420584082603075649218875749838917517148908797769116909790404224468637112682414080.00000000\n",
      "Iteration 178, loss = 111682768682831495904278561408394523784580708096978885661780109524616788312946925988432383328332952861933568.00000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 179, loss = 111626494505352032150961992696365700599305247556460119145654923535285670607079646839369110876108166606618624.00000000\n",
      "Iteration 180, loss = 111570248683035252503908442871712056257673993715639663102003649977931530068552697556615916990719916783960064.00000000\n",
      "Iteration 181, loss = 111514031201593402921395637201235402848749075427927806629400561318082482674718042538683156121691503665872896.00000000\n",
      "Iteration 182, loss = 111457842046746111580079537129314189308767417133190489347878575374238949775916106461911525050769436156362752.00000000\n",
      "Iteration 183, loss = 111401681204220731097038364471565994520016391633280358045619391195163764967071265816530423132490070602285056.00000000\n",
      "Iteration 184, loss = 111345548659751165197050232750861836686117296290216001033158737480601807670793020783029288612236849156456448.00000000\n",
      "Iteration 185, loss = 111289444399078716119559302343538061005431731327496945554238137610760267771802926654778300174310072070963200.00000000\n",
      "Iteration 186, loss = 111233368407951840174358620341065987457578990704724331609597669733958569280080671312272405341522155689476096.00000000\n",
      "Iteration 187, loss = 111177320672126131445302309874163220655870554842575623545228817439138365241740480388080922368503982980071424.00000000\n",
      "Iteration 188, loss = 111121301177364354382881191464571030142441105172855186875868768410843547917278306936948336455092202469588992.00000000\n",
      "Iteration 189, loss = 111065309909437111952023020736490646438434322414531113166633536856310457435639219672860622120768856397971456.00000000\n",
      "Iteration 190, loss = 111009346854121232299599231503602936435017666346231667270050180083957377970981514299055830641990083467542528.00000000\n",
      "Iteration 191, loss = 110953411997200876901996061729499333427836870507964899324862952772704883803081162291451161307354282611179520.00000000\n",
      "Iteration 192, loss = 110897505324467882787158577721344330215891594976649701402723442849265946188927302434701321658168751803072512.00000000\n",
      "Iteration 193, loss = 110841626821720833646185465604220140690301511689674368038603059821215643282677335224325835406911668429520896.00000000\n",
      "Iteration 194, loss = 110785776474765597610826783625453474703968044520731113818448962584161328076735552423372179958122521705840640.00000000\n",
      "Iteration 195, loss = 110729954269415147994318044719839946448353789254537900849965416819352572421393593876861407234771968537395200.00000000\n",
      "Iteration 196, loss = 110674160191489661069107080564976215338875557236990169233096690958621197559572016522090532318423730322669568.00000000\n",
      "Iteration 197, loss = 110618394226816206437385638739374873211159737149680357236831218962071177433471992522674969422055253076869120.00000000\n",
      "Iteration 198, loss = 110562656361229415178889620433900740371228093285934726180834722745168847338642697505614854264497335586258944.00000000\n",
      "Iteration 199, loss = 110506946580570713925371978685002428659918923624624805083334193786712664735172353313926333055832604450750464.00000000\n",
      "Iteration 200, loss = 110451264870688715971510174596040904994459234430774312543049476988593331386654504047851117058043101296197632.00000000\n",
      "Iteration 201, loss = 110395611217440052385584521807612688894305611281848865738297886377784052905491352652426786026386405600526336.00000000\n",
      "Iteration 202, loss = 110339985606686976455170017141912400789014649639038583900163253965309790651725320165078266525429163018420224.00000000\n",
      "Iteration 203, loss = 110284388024299498500839539144151171349324407875570870251372491647437928408229969107219983905915165581770752.00000000\n",
      "Iteration 204, loss = 110228818456155369579876037406667951339588900001685123594548441878188267291586408649205464198071702232498176.00000000\n",
      "Iteration 205, loss = 110173276888138272598325547545284905238006788136825724608276302317942460859364267919733144268614467965681664.00000000\n",
      "Iteration 206, loss = 110117763306139631198944176222952017616392690037449049551037201184834580002840722696440561663742654686429184.00000000\n",
      "Iteration 207, loss = 110062277696058316428017508983750670483998048146568278849759509359931021342775788374997188688653260800655360.00000000\n",
      "Iteration 208, loss = 110006820043800369698467828762785910778897505918323494100117301819900420714311207773247664593749050273038336.00000000\n",
      "Iteration 209, loss = 109951390335277813160843936546312069595706876741135624008988456730243281660948028172532733786002141522952192.00000000\n",
      "Iteration 210, loss = 109895988556410751924448728561373791221533582415968649509836918691502631931490335039190601592434788690558976.00000000\n",
      "Iteration 211, loss = 109840614693126363687494934370707243986915202104761722234389440434883707954382372251432251645113114671710208.00000000\n",
      "Iteration 212, loss = 109785268731358360959607364568413345396157732254594646924979655015083786896629912542678308360256678701039616.00000000\n",
      "Iteration 213, loss = 109729950657047708098486580519060128620217906698800571551422646219852410585238430243776553632759052907839488.00000000\n",
      "Iteration 214, loss = 109674660456142963531952918551345235595578849432497043957705084447281492378808592819060287076754461126819840.00000000\n",
      "Iteration 215, loss = 109619398114599187906663174673553677137361087185891688274926221763974976196255406917971652877143264597114880.00000000\n",
      "Iteration 216, loss = 109564163618378970754242677154545312235949507750875374861368299536919157131594692983236720511291878394560512.00000000\n",
      "Iteration 217, loss = 109508956953451452714016645970433439203062923479504915597667588784082376106527395147841598347413803400953856.00000000\n",
      "Iteration 218, loss = 109453778105793547754595993496236556739167116910899694767850589736165401554709193863812291646593336342478848.00000000\n",
      "Iteration 219, loss = 109398627061388378730247499620564109767186142368813981531607696397560940865885401737376484320195220940521472.00000000\n",
      "Iteration 220, loss = 109343503806226792935660204603264673156094502538988752216778084002924113674385283188650562852374048359317504.00000000\n",
      "Iteration 221, loss = 109288408326306547291554875280991444342641784717887269731992242642672197402942312699119710965391783847002112.00000000\n",
      "Iteration 222, loss = 109233340607632340937259626418979623626483675362745660390166275917964637440941364481738705833005065669967872.00000000\n",
      "Iteration 223, loss = 109178300636215733749270867331602963432354423716446471849766154303253021698800738305679927546996957776969728.00000000\n",
      "Iteration 224, loss = 109123288398076075229658510408029106721300756483960112584395226813211370688017065094206051198714969428328448.00000000\n",
      "Iteration 225, loss = 109068303879238858580997092847461882086565643054790722296332138926245623519682228584579838103012459010850816.00000000\n",
      "Iteration 226, loss = 109013347065736714779922227888351402755084239279518765033106941561383948341022650266138420306561251535749120.00000000\n",
      "Iteration 227, loss = 108958417943610178502657706575162503523062729395989584539617109470306981524208246627662011602453163596054528.00000000\n",
      "Iteration 228, loss = 108903516498906188866536915576615247179951656726986870377389803108267359466981704332737281714384252386148352.00000000\n",
      "Iteration 229, loss = 108848642717678627207500589490011699378107663756067175512237796441259886533737109776420493817545248118669312.00000000\n",
      "Iteration 230, loss = 108793796585988463746687106924234139960883057602787512019983820893431580857632302600691087498864399229124608.00000000\n",
      "Iteration 231, loss = 108738978089903773886720301175633753110191317297730639498207714674571679429712470529502076863701121843068928.00000000\n",
      "Iteration 232, loss = 108684187215499901174587566986917528822162166532757949183717916055011688990146097719284031602782494447894528.00000000\n",
      "Iteration 233, loss = 108629423948858903227854297566932797889914326310149657953148386230965212998022740367231541364619176008744960.00000000\n",
      "Iteration 234, loss = 108574688276070187289888500950326147658610298668591056381097433096638150107173225278268741916557735188496384.00000000\n",
      "Iteration 235, loss = 108519980183229108749109081871116069334822741030997705329872869080088256501540494050715077969126796170362880.00000000\n",
      "Iteration 236, loss = 108465299656439268915567147062998268793270991983081102005840066319315863461606474818391436219841613530136576.00000000\n",
      "Iteration 237, loss = 108410646681811195021633340512361764624014981996300318607902861135573465145380900611537898711018465394688000.00000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 238, loss = 108356021245461688370485416422741282229834942426850465857617580933765515023457515954797818562028674754805760.00000000\n",
      "Iteration 239, loss = 108301423333515079150269661258246395188775465692609900703723542419178817741530879166098474184709618437980160.00000000\n",
      "Iteration 240, loss = 108246852932101955323651661024243693744035937821167730205865404053262132168755965222720016961260068668440576.00000000\n",
      "Iteration 241, loss = 108192310027360922626899854263335320741045324152554960003285293422546721023096407946738466767154335522488320.00000000\n",
      "Iteration 242, loss = 108137794605437023829967896494156027313607963661791518375349322472115857226335801005137095621858268612132864.00000000\n",
      "Iteration 243, loss = 108083306652482341699143655219254708343827338134821927476192112622735012201650706806671157636494287370715136.00000000\n",
      "Iteration 244, loss = 108028846154655787145307671138541430543754477593186553982003853513481789715003923646212713626823937979580416.00000000\n",
      "Iteration 245, loss = 107974413098123588112567478425948136882353178544780261443444782826446078548852330756262574314057377383448576.00000000\n",
      "Iteration 246, loss = 107920007469058507356442692286771519502355654782638566521782018567209808224214336224531191203558198865821696.00000000\n",
      "Iteration 247, loss = 107865629253640184665909033151335512819138190160468695635579694942137055872265368529997016825410070859743232.00000000\n",
      "Iteration 248, loss = 107811278438056815381042826291526378719968387920254291370655343089844570414068144553097509723862632067235840.00000000\n",
      "Iteration 249, loss = 107756955008501271876522060959284451443414439236236770484258351138580561349157098833734385064242918270697472.00000000\n",
      "Iteration 250, loss = 107702658951174900596686277868668941962108318295807523842155757511395884521335980138016373492575688185610240.00000000\n",
      "Iteration 251, loss = 107648390252285077612364967812554415847919691045714654656559851802643278750138626200699505131536003383689216.00000000\n",
      "Iteration 252, loss = 107594148898046952323673313982720639061465193619768838543073152819407910367053611075579706996669721281036288.00000000\n",
      "Iteration 253, loss = 107539934874681997090397041815759154818778284861592654875192118434896920283524304816007371500658692559208448.00000000\n",
      "Iteration 254, loss = 107485748168418805750095142111619100822019102800859718959917462635449673358005020392356863681309585056989184.00000000\n",
      "Iteration 255, loss = 107431588765493256580977977792494110735129537403547566155225648795437809287196962042530502268490378442178560.00000000\n",
      "Iteration 256, loss = 107377456652147241191458051183504482673723663119965155753791242133044845654407828138027507362025705771630592.00000000\n",
      "Iteration 257, loss = 107323351814631174148472848099555461930175859238647286392047906143726961654585415781705308862518871436820480.00000000\n",
      "Iteration 258, loss = 107269274239200864090225188074708732641043413081499220994735731722132020982587412478103109986171153465475072.00000000\n",
      "Iteration 259, loss = 107215223912119827799052340338376418741368553057387641242996441665680294488731862710598418415280027860992000.00000000\n",
      "Iteration 260, loss = 107161200819658133164991465827224083487527432135345170340325788422774098851021934652828780722990056432730112.00000000\n",
      "Iteration 261, loss = 107107204948092985852140801517163574769588389745480755835470929882437979783595334230506114214264267614453760.00000000\n",
      "Iteration 262, loss = 107053236283708729298659660423353025111311951778979669622430429374318712036724305121416706925884156464332800.00000000\n",
      "Iteration 263, loss = 106999294812795932124651033750430203406482423186687356882613893329245014405894317992598923651605314502918144.00000000\n",
      "Iteration 264, loss = 106945380521652610353747391584164275986320933606006066965878170840977931416073683129123063944181139751567360.00000000\n",
      "Iteration 265, loss = 106891493396583314820993285041689158355817029959478702332686991326770548330792239671269068140518466570420224.00000000\n",
      "Iteration 266, loss = 106837633423900190431553040204268374785486649331432565317675670814218321945175019892802394296764781025034240.00000000\n",
      "Iteration 267, loss = 106783800589921542087383418639090325325607478765753977895901971122138632743065903716538984799267067774631936.00000000\n",
      "Iteration 268, loss = 106729994880972698390487583221368863627190840842228567505445014217541054620578144972016366019335231832588288.00000000\n",
      "Iteration 269, loss = 106676216283386321272383500976228409745726331902021746870601119437937451579442673261451212340421438442831872.00000000\n",
      "Iteration 270, loss = 106622464783501737846303705367267654090996023771641889120250681064251769072940705722673023783680484925505536.00000000\n",
      "Iteration 271, loss = 106568740367664956703483106972448247574639971037965616199605317648310039095027341864174524114663450144145408.00000000\n",
      "Iteration 272, loss = 106515043022228863468612721594759083380942298348541259811173663942720441249814701585716438123641488111828992.00000000\n",
      "Iteration 273, loss = 106461372733553546725595883779990099918141345912094629649704357450673404534043821879335453759478817334755328.00000000\n",
      "Iteration 274, loss = 106407729488005613573460198429407294620678363947466900108805766671361393593893673757227501648499443190726656.00000000\n",
      "Iteration 275, loss = 106354113271958564440977186345192597343204180009196243751130425632249025774823843457906911548441095674265600.00000000\n",
      "Iteration 276, loss = 106300524071793167901281929775883743754585866313099464778559468421343188171417213652365568802409167141732352.00000000\n",
      "Iteration 277, loss = 106246961873896890301799698760270120173113985110386904621052402725042859508054144217308980604598980960059392.00000000\n",
      "Iteration 278, loss = 106193426664663700208792223016728481795716501387358980995681317898259049069429334214551498719971997900603392.00000000\n",
      "Iteration 279, loss = 106139918430495306925231303310763403912937335767328107199414233853655183473945029354846575592973175644749824.00000000\n",
      "Iteration 280, loss = 106086437157799025677095612913588872577854911482305910170238539160456439998520098606284612370662888583462912.00000000\n",
      "Iteration 281, loss = 106032982832989651686468925329327651576115491000911397838083163705800331827803440225088741172486616542871552.00000000\n",
      "Iteration 282, loss = 105979555442488971282905794018350577998967957777612306772404098870036555380466136704102881889465460768899072.00000000\n",
      "Iteration 283, loss = 105926154972725338199948474824170618406560627102067603476758515011986857991987989061481391410161257780609024.00000000\n",
      "Iteration 284, loss = 105872781410134243945200299629547023992734000725012578799954987931095216033983336067452997354954310721470464.00000000\n",
      "Iteration 285, loss = 105819434741157731133964492024492485272662504957348465113156123077787651705751640182506466235072008540913664.00000000\n",
      "Iteration 286, loss = 105766114952244605340985706092826104001206085245471185662591496810842299192884220413045778839606269067657216.00000000\n",
      "Iteration 287, loss = 105712822029851070655674642771832308927964989897257602627696480169499603143084450878355656396565868229689344.00000000\n",
      "Iteration 288, loss = 105659555960439148942190414291057911483425564404612541181638754106931825021179061810712944223592441737904128.00000000\n",
      "Iteration 289, loss = 105606316730479254652914630962725149094310400897464358547367903233663847190646123493349512585557100900384768.00000000\n",
      "Iteration 290, loss = 105553104326446544459981809782665092128904708538100339766253966459812032949929803209163080795182951974305792.00000000\n",
      "Iteration 291, loss = 105499918734824404660870859070497335476074870376578417811977393077945315604889659941502412045486082142240768.00000000\n",
      "Iteration 292, loss = 105446759942103147475380224398534786740027861348704100650757096563884791338913057570135464874283602137841664.00000000\n",
      "Iteration 293, loss = 105393627934779131046086112093794398271773855424664897007004390892243444400241043778527895402736575018172416.00000000\n",
      "Iteration 294, loss = 105340522699355916474777047226094167646277242135825793597370592788216508429743583342421322910599128333418496.00000000\n",
      "Iteration 295, loss = 105287444222344055970712334821500165744105031999400505778034080470211400302312825274178154449201011053559808.00000000\n",
      "Iteration 296, loss = 105234392490260473591685254178556311143937580067490517898308619205227526740166499090866456787088913815568384.00000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 297, loss = 105181367489628921540081757793167694254402789627793158829563542478576426808308570195671103397448654004420608.00000000\n",
      "Iteration 298, loss = 105128369206980534236666034338816042331303359554461407964626829128541944944731464269607310085689257637183488.00000000\n",
      "Iteration 299, loss = 105075397628852133506648198374135944130804027705473900396081573287419701691562204426279231893303414776397824.00000000\n",
      "Iteration 300, loss = 105022452741788314504524056856667188797816502125871843619901098300237745102882550098330920754654611328991232.00000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anocondo\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "#3 sklearn MLPClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "clf = MLPClassifier( hidden_layer_sizes= (100, 100, 100,), random_state = 1, max_iter = 300, activation = 'relu', solver = 'sgd', alpha = 1e-4, learning_rate_init =.1, verbose = True).fit(X_train, Y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.03862179487179487\n"
     ]
    }
   ],
   "source": [
    "#3 sklearn MLPClassifier accuracy\n",
    "y_pred = clf.predict(X_val)\n",
    "print(accuracy_score(Y_val, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "##4 keras CNN trial (Optimal)\n",
    "\n",
    "# use keras utils to binarize target variabels\n",
    "y_train = keras.utils.to_categorical(Y_train,27)\n",
    "y_val = keras.utils.to_categorical(Y_val,27)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape input variables\n",
    "X_train_shaped = []\n",
    "for i in X_train:\n",
    "    i = i.reshape(28, 28,1)\n",
    "    X_train_shaped.append(i)\n",
    "X_train_shaped = np.array(X_train_shaped)\n",
    "\n",
    "X_val_shaped = []\n",
    "for i in X_val:\n",
    "    i = i.reshape(28, 28,1)\n",
    "    X_val_shaped.append(i)\n",
    "X_val_shaped = np.array(X_val_shaped)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(99840, 28, 28, 1)\n",
      "(24960, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "# check the shapes of input variables\n",
    "print(X_train_shaped.shape)\n",
    "print(X_val_shaped.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\Anocondo\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Epoch 1/3\n",
      "99840/99840 [==============================] - 684s 7ms/step - loss: 0.5963 - categorical_accuracy: 0.8290\n",
      "Epoch 2/3\n",
      "99840/99840 [==============================] - 534s 5ms/step - loss: 0.3641 - categorical_accuracy: 0.8863\n",
      "Epoch 3/3\n",
      "99840/99840 [==============================] - 482s 5ms/step - loss: 0.3217 - categorical_accuracy: 0.8974\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x2c999fd4648>"
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# build the model\n",
    "model = Sequential()\n",
    "model.add(Conv2D(128, kernel_size=5, activation='relu', input_shape=(28,28,1)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(64, kernel_size=5, activation='relu'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(27, activation='softmax'))\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=[keras.metrics.CategoricalAccuracy()])\n",
    "model.fit(X_train_shaped, y_train,  epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24960/24960 [==============================] - 21s 861us/step\n",
      "Test loss: 0.32664645696297673\n",
      "Test accuracy: 0.899919867515564\n",
      "[0.32664645696297673, 0.899919867515564]\n"
     ]
    }
   ],
   "source": [
    "# check the performance in validation set\n",
    "scores = model.evaluate(X_val_shaped, y_val, verbose=1)\n",
    "print('Test loss:', scores[0])\n",
    "print('Test accuracy:', scores[1])\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating confusion matrix\n",
    "y_pred = model.predict(X_val_shaped)\n",
    "matrix = sklearn.metrics.confusion_matrix(y_val.argmax(axis=1), y_pred.argmax(axis=1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[884   3   1  16  10   0  10   9   0   0   0   0   1   6   5   1  26   4\n",
      "    0   0   4   0   1   0   0   2]\n",
      " [ 10 892   1   7   1   0   5  17   1   1   4   3   0   0   8   1   2   2\n",
      "    0   0   0   0   0   0   0   2]\n",
      " [  2   1 851   0  28   0   5   0   0   0   0   4   0   1   3   0   2   6\n",
      "    1   0   0   0   0   1   0   2]\n",
      " [ 15   3   2 863   1   0   0   2   1   4   0   2   0   0  29   3   2   1\n",
      "    0   0   0   1   0   0   0   0]\n",
      " [ 10   3  28   0 879   2   3   1   1   2   0   1   0   0   0   0   0   6\n",
      "    0   0   1   0   1   1   1   4]\n",
      " [  2   0   3   2   4 849   5   0   2   1   0   1   0   0   0   8   1   5\n",
      "    1  26   0   0   0   0   0   1]\n",
      " [ 24  21   6   3   2   4 751   1   1   3   2   3   1   1   1   1 113   1\n",
      "   10   1   0   0   0   0   4   3]\n",
      " [ 13   7   0   3   0   1   1 876   1   0  10   9   5  20   0   0   2   2\n",
      "    0   2   1   0   0   4   0   0]\n",
      " [  0   0   0   0   1   1   0   0 773  10   0 131   0   0   0   0   3   4\n",
      "    0   4   0   0   0   1   0  11]\n",
      " [  6   5   0  21   0   5   6   3  29 891   0   0   0   0   1   0   5   0\n",
      "   16   9   3   1   0   0   3   1]\n",
      " [  1   0   0   0   1   1   0  15   0   0 894   4   0   3   0   0   0   9\n",
      "    0   1   2   0   0   7   2   1]\n",
      " [  0   1   9   3   3   1   0   3 306   0   3 629   0   0   0   1   0   4\n",
      "    0   1   0   0   0   1   1   1]\n",
      " [  2   0   0   0   0   0   1   8   0   1   1   0 957   8   0   0   1   0\n",
      "    0   0   2   0   1   0   0   0]\n",
      " [ 17   1   0   2   0   0   0   9   0   1   3   0   7 863   2   1   2  11\n",
      "    0   0   6   2  19   4   2   0]\n",
      " [  4   0   0  38   0   0   0   0   0   0   0   0   1   0 920   1   2   0\n",
      "    0   0   3   0   0   0   0   0]\n",
      " [  6   1   0   2   2   7   2   0   1   0   0   1   0   2   0 915   7   4\n",
      "    0   1   0   0   0   0   1   0]\n",
      " [ 37   3   0   8   5   0  92   0   1   0   0   1   0   2   2   2 783   0\n",
      "    0   2   0   0   0   1   3   1]\n",
      " [ 17   2   2   0   1   3   1   5   4   1   6   0   1   5   0   3   4 867\n",
      "    1  17   1   5   0   1  11   1]\n",
      " [  0   4   0   1   0   2  32   0   4   7   0   0   0   0   1   0   0   1\n",
      "  932   0   0   0   0   0   0   3]\n",
      " [  2   2   4   1   8  16   1   2   7   6   2   0   0   0   0   0   2   5\n",
      "    1 866   0   0   0   0   4   5]\n",
      " [  3   1   0   3   0   0   0   0   0   1   6   0   1   5   2   0   3   1\n",
      "    0   0 884  39  10   1   4   0]\n",
      " [  3   0   0   2   0   0   1   1   2   3   0   0   0   4   2   0   1  13\n",
      "    0   3  27 937   4   0  13   0]\n",
      " [  3   0   0   1   0   0   1   4   0   2   0   0   2  14   0   0   1   0\n",
      "    1   0   8   1 928   2   1   0]\n",
      " [  8   1   0   2   0   0   0   2   1   0  22   0   0   1   0   0   0   4\n",
      "    0   2   0   0   0 912   6   2]\n",
      " [  2   0   0   3   0   1   9   2   3   3   1   0   0   1   0   3   2   6\n",
      "    0   7   4  12   1   9 895   0]\n",
      " [  1   4   0   1   5   3   4   0   0   1   1   0   0   0   0   1   3   0\n",
      "    0   1   0   0   0   0   1 983]]\n"
     ]
    }
   ],
   "source": [
    "print(matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "99840/99840 [==============================] - 178s 2ms/step - loss: 0.6255 - accuracy: 0.8291\n",
      "Epoch 2/3\n",
      "99840/99840 [==============================] - 182s 2ms/step - loss: 0.3231 - accuracy: 0.8975\n",
      "Epoch 3/3\n",
      "99840/99840 [==============================] - 173s 2ms/step - loss: 0.2344 - accuracy: 0.9211\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x2c9b9530488>"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# try a smaller model to make it faster in testing task. The difference in accuracy is very small. \n",
    "model = Sequential()\n",
    "model.add(Conv2D(64, kernel_size=3, activation='relu', input_shape=(28,28,1)))\n",
    "model.add(Conv2D(32, kernel_size=3, activation='relu'))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(27, activation='softmax'))\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(X_train_shaped, y_train,  epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24960/24960 [==============================] - 6s 235us/step\n",
      "Test loss: 0.3934878533395628\n",
      "Test accuracy: 0.8832532167434692\n"
     ]
    }
   ],
   "source": [
    "# check the performance in validation set\n",
    "scores = model.evaluate(X_val_shaped, y_val, verbose=1)\n",
    "print('Test loss:', scores[0])\n",
    "print('Test accuracy:', scores[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with np.load('training-dataset.npz') as data:\n",
    "    img = data['x']\n",
    "    lbl = data['y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img1 = img\n",
    "img2 = img\n",
    "img3 = img\n",
    "type(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "img3 = scaler.fit_transform(img3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.util import random_noise\n",
    "img2 = np.array(img2)\n",
    "#add noise to the entire image dataset\n",
    "for i in range(len(img3)):\n",
    "    img3[i,:] = random_noise(img3[i,:], mode='s&p',amount=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "img3 = scaler.inverse_transform(img3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAPoElEQVR4nO3df4xV9ZnH8c/DMDLE+gf4Ayd0tC2QKG4i3Uxwo83qphEp/yAmriVmwyYqNamxTTBK3D9qYjZB3Wr6x6YJrqb4YzXGQjCGWAhpdBtNw2j8gQsKktnOVJACYiEwlBme/WOO7aj3fM9wz7n33PF5v5LJvXOfe+55cuEz59z7Ped8zd0F4OtvWt0NAGgPwg4EQdiBIAg7EARhB4KY3s6VdXV1eXd3dztXCYRy+vRpjY2NWaNaqbCb2VJJv5DUJem/3H1d6vnd3d3q6+srs0oACUNDQ7m1pnfjzaxL0n9K+oGkhZJWmtnCZl8PQGuV+cy+WNJed9/n7n+R9Lyk5dW0BaBqZcI+V9LEfYbh7LEvMLPVZjZgZgNjY2MlVgegjDJhb/QlwFeOvXX39e7e7+79XV1dJVYHoIwyYR+WNPHbtm9K+rhcOwBapUzYd0haYGbfNrNzJP1Q0kvVtAWgak0Pvbn7qJndJek3Gh96e9Ld36+sM6BG06enozE6OtqmTqpTapzd3bdI2lJRLwBaiMNlgSAIOxAEYQeCIOxAEIQdCIKwA0G09Xx2SUpdzdas4Wm4QNtNxXH0ImzZgSAIOxAEYQeCIOxAEIQdCIKwA0G0feiN4TUgreiKTs1e3o0tOxAEYQeCIOxAEIQdCIKwA0EQdiAIwg4E0fZx9rpceeWVyfo777zTpk6AtFZNk8aWHQiCsANBEHYgCMIOBEHYgSAIOxAEYQeCCDPOzjh6a2zZkp7Ed9myZU2/9iuvvJKsL126tOnXjqhU2M1sUNIxSWOSRt29v4qmAFSvii37P7n7oQpeB0AL8ZkdCKJs2F3SVjN708xWN3qCma02swEzG2jVMb8AipXdjb/G3T82s4skbTOz3e7+2sQnuPt6SeslqaenJ3+iNwAtVWrL7u4fZ7cHJW2StLiKpgBUr+mwm9m5Znbe5/clLZG0s6rGAFTLUlMoJxc0+47Gt+bS+MeB/3b3f08t09PT4319fU2tD5hKbrrppmR948aNLVnv0NCQRkZGGk7O0PRndnffJyl9RQgAHYOhNyAIwg4EQdiBIAg7EARhB4JoeuitGWWH3o4cOZJbmz17dnLZvXv3Juvz589vqqevu61btybrS5YsaVMnmIzU0BtbdiAIwg4EQdiBIAg7EARhB4Ig7EAQhB0IYkqNs9fpqquuyq198MEHyWWPHj1adTuo2bRp6e2kWcOh7r9q1SXaGGcHQNiBKAg7EARhB4Ig7EAQhB0IgrADQUypKZsPHz6cWzv//PNLvXbR8rfeemtubc6cOcllb7nllqZ6mgp6enqS9dHR0aZqdXvssceS9e3btyfrRePsqesEnDp1Krlss9iyA0EQdiAIwg4EQdiBIAg7EARhB4Ig7EAQU2qcvexYesrdd9+drM+cOTO3VjTWfPHFFyfrBw4cSNbL6OrqStZnzZqVrF9//fXJ+u23356sP/7447m1559/PrlsWZdeemlurejYh9OnTyfr8+bNS9b37NmTrPf29ubWBgcHk8s2q3DLbmZPmtlBM9s54bHZZrbNzPZkt+n/MQBqN5nd+F9JWvqlx9ZK2u7uCyRtz34H0MEKw+7ur0n68rxLyyVtyO5vkHRjxX0BqFizn9nnuPt+SXL3/WZ2Ud4TzWy1pNWSNH36lPqKAPhaafm38e6+3t373b2/6MsiAK3TbNg/MbNeScpuD1bXEoBWaDbsL0lald1fJWlzNe0AaJXCD9Fm9pyk6yRdYGbDkn4maZ2kF8zsNkl/kHRzK5uswowZM1pW7+7uTi5b53Xjb7jhhmT95pvT/3SXX355sn7mzJlk/Y477sitbdy4Mbnseeedl6wvWrQoWX/22Wdza/fee29y2U8//TRZLzoXv+h9qUNh2N19ZU7p+xX3AqCFOFwWCIKwA0EQdiAIwg4EQdiBIDrq+NWiI+xS09wWnWa6YsWKZL1o6O2cc87JrT399NPJZUdGRpL1sqehLlmyJLf2wgsvJJddt25dsl50SeSiU0FT/2aPPPJIctlLLrkkWT958mSyvnJl3kCSdOzYseSyTzzxRLJeNKRZNCVzaqr0omG9Zg87Z8sOBEHYgSAIOxAEYQeCIOxAEIQdCIKwA0F01Dh70dhkStEljx988MFkfdu2bcn6oUOHcmtF0/deccUVyfo999yTrC9cuDBZT43Tr12bvhZo0fEJRcqcyrlgwYJk/cSJE8n6ww8/nKwPDw/n1opOYS06vqDMOLqUft9Sx3QULZvClh0IgrADQRB2IAjCDgRB2IEgCDsQBGEHguiocfYiqTHhorHJp556KlkvOp/9mWeeya1dffXVyWXXrFmTrBedn1w0ZpsybVr673nReHLRmG7RePO+fftyay+++GJy2TfeeCNZTx37UOShhx5K1ovOtS96X8r8m7XqMtRs2YEgCDsQBGEHgiDsQBCEHQiCsANBEHYgiCk1zj537tzcWtE4e5nrwkvpc85nzpyZXLZoLPvDDz9M1nfs2JGsb968ObeWOj5Aki688MJkfcOGDcn6yy+/nKzv3r07t1bm+gVl3Xfffcl60XTQRWPhnThlc+GW3cyeNLODZrZzwmMPmNkfzezt7GdZa9sEUNZkduN/JWlpg8cfc/dF2c+WatsCULXCsLv7a5KOtKEXAC1U5gu6u8zs3Ww3P3cyMjNbbWYDZjZQ52c0ILpmw/5LSfMkLZK0X9LP857o7uvdvd/d+4smMATQOk2F3d0/cfcxdz8j6XFJi6ttC0DVmgq7mfVO+HWFpJ15zwXQGQrH2c3sOUnXSbrAzIYl/UzSdWa2SJJLGpT0oyqaKdrNT52b3epx9u7u7txaUd933nlnsr5r165kvcx3HUXz0hcdA3D48OGm1z2VFc2/fuDAgVKvXzSvfSsUht3dG81on56pHkDH4XBZIAjCDgRB2IEgCDsQBGEHguioU1yLhphSl1xODY1J5YbWpPRljV9//fXksjt31ncYwpEjnNbQSN1Hc548ebLt62TLDgRB2IEgCDsQBGEHgiDsQBCEHQiCsANBdNQ4e5Hjx4/n1orGTYeGhpL1Rx99NFkfHBxM1lOKpk3uxMsOVyU1zj979uw2dnJ2ik79Lfo3LZqy+bPPPjvrnspiyw4EQdiBIAg7EARhB4Ig7EAQhB0IgrADQVjReGCVenp6vK+vryWvnTrXfTJGR0dbtu6iMdmi8/iZNqv9LrvssmS96PoHRcd1HD169Kx7moyhoSGNjIw0PEiALTsQBGEHgiDsQBCEHQiCsANBEHYgCMIOBDGlzmdPKTNO3mqRz2efqnbv3l13C5Ur3LKbWZ+Z/dbMdpnZ+2b2k+zx2Wa2zcz2ZLezWt8ugGZNZjd+VNIad79c0j9I+rGZLZS0VtJ2d18gaXv2O4AOVRh2d9/v7m9l949J2iVprqTlkjZkT9sg6cZWNQmgvLP6zG5m35L0XUm/lzTH3fdL438QzOyinGVWS1otlT9+HUDzJv1tvJl9Q9KvJf3U3f882eXcfb2797t7f92T6QGRTSrsZtat8aA/6+4bs4c/MbPerN4r6WBrWgRQhcL9ahu/pu4Tkna5+8TrLb8kaZWkddnt5pZ0OAUUDfsVDb3h66e/vz9ZHxgYaFMnfzOZD9HXSPoXSe+Z2dvZY/drPOQvmNltkv4g6ebWtAigCoVhd/ffScq7Yv73q20HQKuwfwkEQdiBIAg7EARhB4Ig7EAQHL/aBidOnEjWyx5GfOrUqdzajBkzSr02mlPHOHoRtuxAEIQdCIKwA0EQdiAIwg4EQdiBIAg7EESYcfa9e/cm6/Pnz2/ZusuOo2/atClZX7FiRanXR/XmzZuXrH/00Udt6uRv2LIDQRB2IAjCDgRB2IEgCDsQBGEHgiDsQBDm7m1bWU9Pj/f19bVtfUA0Q0NDGhkZaXg1aLbsQBCEHQiCsANBEHYgCMIOBEHYgSAIOxDEZOZn75P0lKSLJZ2RtN7df2FmD0i6Q9Kfsqfe7+5bWtUo8l177bW5tVdffbWNnaCTTeaqCqOS1rj7W2Z2nqQ3zWxbVnvM3f+jde0BqMpk5mffL2l/dv+Yme2SNLfVjQGo1ll9Zjezb0n6rqTfZw/dZWbvmtmTZjYrZ5nVZjZgZgNjY2OlmgXQvEmH3cy+IenXkn7q7n+W9EtJ8yQt0viW/+eNlnP39e7e7+79XV1dFbQMoBmTCruZdWs86M+6+0ZJcvdP3H3M3c9IelzS4ta1CaCswrCbmUl6QtIud390wuO9E562QtLO6tsDUJXCU1zN7HuS/kfSexofepOk+yWt1PguvEsalPSj7Mu8XGVPcZ02Lf9v05kzZ3JrQNUmkZs2dfJFqVNcJ/Nt/O8kNVqYMXVgCuEIOiAIwg4EQdiBIAg7EARhB4Ig7EAQU2rKZsbS0SnqGkcvgy07EARhB4Ig7EAQhB0IgrADQRB2IAjCDgTR1imbzexPkv5vwkMXSDrUtgbOTqf21ql9SfTWrCp7u9TdL2xUaGvYv7JyswF376+tgYRO7a1T+5LorVnt6o3deCAIwg4EUXfY19e8/pRO7a1T+5LorVlt6a3Wz+wA2qfuLTuANiHsQBC1hN3MlprZB2a218zW1tFDHjMbNLP3zOxtMxuouZcnzeygme2c8NhsM9tmZnuy24Zz7NXU2wNm9sfsvXvbzJbV1Fufmf3WzHaZ2ftm9pPs8Vrfu0RfbXnf2v6Z3cy6JH0o6XpJw5J2SFrp7v/b1kZymNmgpH53r/0ADDP7R0nHJT3l7n+XPfawpCPuvi77QznL3e/rkN4ekHS87mm8s9mKeidOMy7pRkn/qhrfu0Rf/6w2vG91bNkXS9rr7vvc/S+Snpe0vIY+Op67vybpyJceXi5pQ3Z/g8b/s7RdTm8dwd33u/tb2f1jkj6fZrzW9y7RV1vUEfa5koYm/D6szprv3SVtNbM3zWx13c00MOfzabay24tq7ufLCqfxbqcvTTPeMe9dM9Ofl1VH2BtdvKuTxv+ucfe/l/QDST/OdlcxOZOaxrtdGkwz3hGanf68rDrCPixp4uyO35T0cQ19NOTuH2e3ByVtUudNRf3J5zPoZrcHa+7nrzppGu9G04yrA967Oqc/ryPsOyQtMLNvm9k5kn4o6aUa+vgKMzs3++JEZnaupCXqvKmoX5K0Kru/StLmGnv5gk6ZxjtvmnHV/N7VPv25u7f9R9IyjX8j/5Gkf6ujh5y+viPpnezn/bp7k/ScxnfrTmt8j+g2SedL2i5pT3Y7u4N6e1rjU3u/q/Fg9dbU2/c0/tHwXUlvZz/L6n7vEn215X3jcFkgCI6gA4Ig7EAQhB0IgrADQRB2IAjCDgRB2IEg/h92rffv0NqN6QAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "d =img3[6]\n",
    "d = d.reshape(28,28)\n",
    "pt.imshow(d, cmap = \"gray\")\n",
    "pt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(99840, 784)\n",
      "(24960, 784)\n",
      "(99840,)\n",
      "(24960,)\n"
     ]
    }
   ],
   "source": [
    "# split datasets\n",
    "X_train, X_val, Y_train, Y_val= train_test_split(img3, lbl, test_size=0.2)\n",
    "print(X_train.shape)\n",
    "print(X_val.shape)\n",
    "print(Y_train.shape)\n",
    "print(Y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784,)\n",
      "(28, 28)\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as pt\n",
    "from scipy import misc\n",
    "d = img[2]\n",
    "print(d.shape)\n",
    "d = d.reshape(28,28)\n",
    "print(d.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "##4 keras CNN trial (Optimal)\n",
    "\n",
    "# one_hot \n",
    "y_train = keras.utils.to_categorical(Y_train,27)\n",
    "y_val = keras.utils.to_categorical(Y_val,27)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_shaped = []\n",
    "for i in X_train:\n",
    "    i = i.reshape(28, 28,1)\n",
    "    X_train_shaped.append(i)\n",
    "X_train_shaped = np.array(X_train_shaped)\n",
    "\n",
    "X_val_shaped = []\n",
    "for i in X_val:\n",
    "    i = i.reshape(28, 28,1)\n",
    "    X_val_shaped.append(i)\n",
    "X_val_shaped = np.array(X_val_shaped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(99840, 28, 28, 1)\n",
      "(24960, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "print(X_train_shaped.shape)\n",
    "print(X_val_shaped.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\Anocondo\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Epoch 1/3\n",
      "99840/99840 [==============================] - 177s 2ms/step - loss: 0.8008 - accuracy: 0.7744\n",
      "Epoch 2/3\n",
      "99840/99840 [==============================] - 180s 2ms/step - loss: 0.4379 - accuracy: 0.8634\n",
      "Epoch 3/3\n",
      "99840/99840 [==============================] - 177s 2ms/step - loss: 0.3440 - accuracy: 0.8902\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x2122ae6a848>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(64, kernel_size=3, activation='relu', input_shape=(28,28,1)))\n",
    "model.add(Conv2D(32, kernel_size=3, activation='relu'))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(27, activation='softmax'))\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(X_train_shaped, y_train,  epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24960/24960 [==============================] - 6s 246us/step\n",
      "Test loss: 0.4744233191968539\n",
      "Test accuracy: 0.8551282286643982\n"
     ]
    }
   ],
   "source": [
    "scores = model.evaluate(X_val_shaped, y_val, verbose=1)\n",
    "print('Test loss:', scores[0])\n",
    "print('Test accuracy:', scores[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "test = X_val_shaped[1:2]\n",
    "test = test.reshape(28, 28)\n",
    "pt.imshow(test, cmap = \"gray\")\n",
    "pt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24960/24960 [==============================] - 6s 246us/step\n",
      "Test loss: 0.4744233191968539\n",
      "Test accuracy: 0.8551282286643982\n",
      "[0.4744233191968539, 0.8551282286643982]\n"
     ]
    }
   ],
   "source": [
    "# check the performance in validation set\n",
    "scores = model.evaluate(X_val_shaped, y_val, verbose=1)\n",
    "print('Test loss:', scores[0])\n",
    "print('Test accuracy:', scores[1])\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_val_shaped)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix = sklearn.metrics.confusion_matrix(y_val.argmax(axis=1), y_pred.argmax(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.7951265e-11, 3.4154598e-07, 4.1213008e-07, ..., 6.3371161e-05,\n",
       "        7.6759339e-04, 1.6662248e-11],\n",
       "       [1.0382462e-07, 6.4845737e-03, 8.8985216e-06, ..., 3.8728590e-10,\n",
       "        2.3617950e-09, 2.6382891e-08],\n",
       "       [3.1541911e-10, 3.0956820e-05, 1.5725789e-06, ..., 5.7560906e-06,\n",
       "        2.8682618e-06, 2.5887357e-05],\n",
       "       ...,\n",
       "       [2.5664079e-08, 3.4028240e-02, 3.1183688e-05, ..., 1.1990814e-07,\n",
       "        2.5567379e-05, 3.2337351e-08],\n",
       "       [9.2693474e-13, 1.8899220e-09, 4.8521129e-09, ..., 8.4723895e-12,\n",
       "        2.2841871e-08, 4.2444381e-10],\n",
       "       [4.1103568e-11, 8.9812456e-05, 1.3627206e-07, ..., 2.7151287e-10,\n",
       "        5.1490051e-08, 2.3361182e-10]], dtype=float32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[789,   3,   8,  15,   5,   6,  13,  13,   1,   0,   2,   0,   4,\n",
       "         31,  11,   2,  36,   5,   0,   2,   5,   0,   1,   2,   0,   1],\n",
       "       [ 10, 897,   1,  18,   0,   0,  20,  12,   5,   1,   3,   4,   0,\n",
       "          3,   5,   1,   5,   0,   0,   0,   0,   0,   1,   2,   0,   3],\n",
       "       [  4,   2, 860,   3,  14,   1,  11,   1,   2,   0,   1,   3,   0,\n",
       "          1,   6,   0,   0,   8,   0,   0,   2,   0,   0,   0,   1,   0],\n",
       "       [ 11,   8,   2, 778,   2,   0,   3,   3,   0,  17,   3,   6,   0,\n",
       "          6,  36,   6,   4,   1,   0,   1,   4,   6,   1,   2,   1,   0],\n",
       "       [  8,   5,  86,   2, 798,  11,   7,   0,   3,   4,   2,   2,   0,\n",
       "          1,   2,   0,   4,  14,   2,   2,   1,   1,   0,   1,   0,   1],\n",
       "       [  0,   1,   2,   1,   3, 879,  16,   0,   8,   3,   3,   1,   0,\n",
       "          0,   0,  15,   5,  16,   3,  20,   0,   0,   2,   1,   1,   1],\n",
       "       [ 18,  24,  22,   4,   5,  10, 693,   0,   3,  10,   4,   1,   1,\n",
       "          0,   2,   1, 110,   2,  21,   0,   2,   1,   1,   2,  13,   2],\n",
       "       [ 17,  11,   0,   9,   0,   0,   5, 791,   4,   0,  20,  12,   3,\n",
       "         58,   0,   0,   1,   3,   0,   1,   9,   3,   7,   5,   2,   0],\n",
       "       [  2,   0,   1,   3,   0,   1,   8,   3, 684,  30,   0, 196,   0,\n",
       "          3,   1,   3,   0,   4,   1,   5,   2,   4,   0,   5,   1,   8],\n",
       "       [  0,   2,   1,  22,   0,   3,  17,   1,  27, 786,   0,   3,   0,\n",
       "          3,   0,   0,   3,   1,   9,   8,   2,   4,   0,   5,   6,   1],\n",
       "       [  2,   5,   2,   2,   0,   3,   1,  13,   3,   0, 886,  10,   0,\n",
       "          4,   0,   3,   2,  27,   0,   4,   3,   1,   0,  32,   0,   0],\n",
       "       [  0,   5,  14,  10,   1,   1,   0,   1, 345,   5,   5, 574,   0,\n",
       "          1,   0,   1,   0,   0,   0,   0,   4,   1,   0,   4,   2,   2],\n",
       "       [  5,   0,   0,   0,   0,   0,   1,   7,   0,   0,   0,   0, 905,\n",
       "         63,   0,   0,   1,   1,   0,   1,   1,   1,   7,   1,   1,   0],\n",
       "       [ 20,   1,   0,   5,   1,   0,   1,   8,   0,   0,   3,   0,  13,\n",
       "        843,   1,   1,   1,   3,   0,   0,   2,   7,  12,   4,   1,   0],\n",
       "       [  6,   1,   2,  29,   2,   0,  11,   0,   0,   0,   0,   0,   0,\n",
       "          1, 925,   3,   5,   1,   0,   0,   3,   3,   0,   0,   0,   0],\n",
       "       [  7,   0,   1,   6,   0,  18,   1,   0,   0,   0,   0,   2,   0,\n",
       "          4,   3, 879,  10,  31,   0,   4,   0,   3,   0,   0,   2,   0],\n",
       "       [ 42,   3,   2,  13,   0,   3, 186,   0,   4,   1,   1,   1,   1,\n",
       "          5,  10,   7, 660,   1,   1,  10,   0,   1,   2,   0,   8,   2],\n",
       "       [ 38,   0,  14,   1,   2,   7,   2,   2,   3,   0,  11,   2,   0,\n",
       "         13,   0,  13,   5, 813,   0,  14,   0,  18,   0,   6,   1,   1],\n",
       "       [  2,   6,   1,   2,   0,   3,  27,   0,  10,  27,   0,   0,   0,\n",
       "          1,   2,   0,   2,   1, 881,   0,   0,   0,   0,   0,   0,   0],\n",
       "       [  0,   6,   4,   4,   4,  28,   4,   2,  17,   9,   9,   6,   0,\n",
       "          3,   1,   1,   3,  16,   0, 740,   0,   1,   0,   7,  20,   4],\n",
       "       [ 11,   2,   1,   4,   1,   0,   2,   0,   1,   2,   5,   1,   1,\n",
       "          1,   3,   0,   2,   1,   1,   0, 892,  32,   8,   2,   4,   0],\n",
       "       [  0,   0,   1,   1,   1,   0,   4,   0,   0,   0,   1,   2,   0,\n",
       "         13,   0,   0,   0,   8,   1,   1,  45, 870,   7,   3,  11,   0],\n",
       "       [  3,   2,   2,   4,   1,   2,   1,   2,   0,   0,   0,   0,   0,\n",
       "         41,   0,   0,   0,   0,   0,   0,   9,   9, 851,   0,   0,   0],\n",
       "       [  5,   0,   0,   1,   0,   0,   1,   2,   0,   0,  12,   0,   0,\n",
       "          7,   0,   1,   1,   8,   0,   4,   1,   1,   0, 896,  16,   3],\n",
       "       [  2,   1,   0,   3,   0,   2,  11,   2,   6,   2,   2,   5,   1,\n",
       "          0,   1,   4,   6,   5,   0,   5,   7,  58,   1,  15, 862,   0],\n",
       "       [ 13,   2,   0,   2,   4,   1,   5,   0,   3,   1,   0,   2,   1,\n",
       "          1,   0,   2,   4,   2,   3,   9,   6,   0,   0,  18,   1, 912]],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30, 168)\n"
     ]
    }
   ],
   "source": [
    "# loading data\n",
    "testdata = np.load('test-dataset.npy')\n",
    "print(testdata[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sliding_window(image):\n",
    "    blank = []\n",
    "    for i in range(0, 140, 10):\n",
    "        crops = image[0:28, i:i+28]\n",
    "        blank.append(crops)\n",
    "    return blank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(fin_result):\n",
    "    result_list = []\n",
    "    for i in fin_result:\n",
    "        for j in range(14):\n",
    "            encoded = np.where(np.round(i)[j] == 1.0)\n",
    "            result_list.append(encoded)\n",
    "    return result_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_five(result):\n",
    "    max1 = []\n",
    "    max_index = []\n",
    "    fin_result = []\n",
    "    for i in result:\n",
    "        max1.append(np.max(i))\n",
    "    max1 = enumerate(max)\n",
    "    max1 = sorted(max, key=lambda x: x[1])\n",
    "    max1 = max1[-6:-1]\n",
    "    max1 = sorted(max, key=lambda x: x[0])\n",
    "    for i in max1:\n",
    "        max_index.append(i[0])\n",
    "    for i in max_index:\n",
    "        fin_result.append(result[i])\n",
    "    fin_result = np.round(fin_result)\n",
    "    return fin_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## combine everthing here\n",
    "# crop the pics\n",
    "testdata_cropped = []\n",
    "testdata_cropped_shaped = []\n",
    "for i in testdata:\n",
    "    testdata_cropped.append(sliding_window(i))\n",
    "testdata_cropped = np.array(testdata_cropped)\n",
    "for i in testdata_cropped:\n",
    "    i = i.reshape(14,28,28,1)\n",
    "    testdata_cropped_shaped.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction\n",
    "itr = 0\n",
    "result_list = []\n",
    "while itr <= 4:\n",
    "    itr += 1\n",
    "    result = []\n",
    "    best_result = []\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(64, kernel_size=3, activation='relu', input_shape=(28,28,1)))\n",
    "    model.add(Conv2D(32, kernel_size=3, activation='relu'))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(27, activation='softmax'))\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    model.fit(X_train_shaped, y_train,  epochs=1)\n",
    "    for i in testdata_cropped_shaped:\n",
    "        i = i.reshape(14,28,28,1)\n",
    "        result.append(model.predict(i))\n",
    "    for i in result:\n",
    "        best_result.append(best_five(i))\n",
    "    result_list.append(best_result)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making 5 predictions\n",
    "result = list(zip(result_list[0], result_list[1], result_list[2], result_list[3], result_list[4]))\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a small accuracy score evaluation. \n",
    "correct = 0\n",
    "for i in range(0, len(result)-1):\n",
    "    for j in result[i]:\n",
    "        if any(np.round(j)== y_val[i]):\n",
    "            correct += 1\n",
    "print(correct/len(result))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
